from tqdm import tqdm
import numpy as np
import torch

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


'''EMBEDDINGS USING BERT AND PYTORCH HUB
Use PyTorch and the Transformers library by Hugging Face to tokenize text, convert it to embeddings using BERT, and handle these embeddings with a model.
'''

# load tokenizer and model
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')   
'''BERT model (Bidirectional Encoder Representations from Transformers) introduced in 2018: 2 models - base or large; uncased changes everything to lower case'''

# input text to get embeddings for - set as a list of tuples
input_text = [("This is an example sentence for BERT embeddings.", "How do you like it "),("There are other models")]

# tokenise
input_ids = tokenizer.batch_encode_plus(input_text,add_special_tokens=True,padding=True,truncation=True)
print("tokenised input_ids:\n", input_ids)
'''batch_encode_plus method is used for tokenizing text. It automatically handles padding and truncation to ensure uniformity in input length, which is crucial for batch processing in models like BERT.

add_special_tokens=True:
For BERT (bert-base-uncased):
[CLS] — classification token added at the start
[SEP] — separator token added at the end (and between sentences if there are two)
'''